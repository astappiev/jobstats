{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"What is Jobstats?","text":"<p>Jobstats is a free and open-source job monitoring platform designed for CPU and GPU clusters that use the Slurm workload manager. It was released in 2023 under the GNU GPL v2 license. Visit the Jobstats GitHub repository.</p>"},{"location":"#what-are-the-main-benefits-of-jobstats-over-other-platforms","title":"What are the main benefits of Jobstats over other platforms?","text":"<p>The main advantages of Jobstats are:</p> <ul> <li>utilization and memory usage for each allocated GPU</li> <li>automatically cancel jobs with 0% GPU utilization</li> <li>accurate CPU memory usage for jobs of any size</li> <li>graphical interface for inspecting job metrics versus time</li> <li>efficiency reports contain job-specific notes to guide users</li> <li>automated emails to users for instances of underutilization</li> <li>periodic reports on usage and efficiency for users and group leaders</li> <li>all of the above features work with Open OnDemand jobs</li> </ul>"},{"location":"#how-does-jobstats-work","title":"How does Jobstats work?","text":"<p>A schematic diagram of the components of the Jobstats platform and the external tools is shown below:</p> <p></p> <p>A compute node with two sockets is shown in the upper left. The dotted line around the node indicates the three node-level exporters, namely, Node, cgroups and NVIDIA. A GPFS server is shown in the upper right with its cluster-level GPFS exporter. The exporters serve to make data available to the Prometheus database. Users interact with the Prometheus and Slurm data via the web interface (i.e., Grafana) and external tools (e.g., <code>jobstats</code>).</p>"},{"location":"#which-institutions-are-using-jobstats","title":"Which institutions are using Jobstats?","text":"<p>Jobstats is used by these institutions:</p> <ul> <li>Brown University - Center for Computation and Visualization</li> <li>Free University of Berlin - High-Performance Computing</li> <li>Princeton University - Computer Science Department</li> <li>Princeton University - Research Computing</li> <li>Yale University - Center for Research Computing</li> <li>and more</li> </ul>"},{"location":"#what-does-a-jobstats-efficiency-report-look-like","title":"What does a Jobstats efficiency report look like?","text":"<p>The <code>jobstats</code> command generates a job report:</p> <pre><code>$ jobstats 39798795\n\n================================================================================\n                              Slurm Job Statistics\n================================================================================\n         Job ID: 39798795\n  NetID/Account: aturing/math\n       Job Name: sys_logic_ordinals\n          State: COMPLETED\n          Nodes: 2\n      CPU Cores: 48\n     CPU Memory: 256GB (5.3GB per CPU-core)\n           GPUs: 4\n  QOS/Partition: della-gpu/gpu\n        Cluster: della\n     Start Time: Fri Mar 4, 2022 at 1:56 AM\n       Run Time: 18:41:56\n     Time Limit: 4-00:00:00\n\n                              Overall Utilization\n================================================================================\n  CPU utilization  [|||||                                          10%]\n  CPU memory usage [|||                                             6%]\n  GPU utilization  [||||||||||||||||||||||||||||||||||             68%]\n  GPU memory usage [|||||||||||||||||||||||||||||||||              66%]\n\n                              Detailed Utilization\n================================================================================\n  CPU utilization per node (CPU time used/run time)\n      della-i14g2: 1-21:41:20/18-16:46:24 (efficiency=10.2%)\n      della-i14g3: 1-18:48:55/18-16:46:24 (efficiency=9.5%)\n  Total used/runtime: 3-16:30:16/37-09:32:48, efficiency=9.9%\n\n  CPU memory usage per node - used/allocated\n      della-i14g2: 7.9GB/128.0GB (335.5MB/5.3GB per core of 24)\n      della-i14g3: 7.8GB/128.0GB (334.6MB/5.3GB per core of 24)\n  Total used/allocated: 15.7GB/256.0GB (335.1MB/5.3GB per core of 48)\n\n  GPU utilization per node\n      della-i14g2 (GPU 0): 65.7%\n      della-i14g2 (GPU 1): 64.5%\n      della-i14g3 (GPU 0): 72.9%\n      della-i14g3 (GPU 1): 67.5%\n\n  GPU memory usage per node - maximum used/total\n      della-i14g2 (GPU 0): 26.5GB/40.0GB (66.2%)\n      della-i14g2 (GPU 1): 26.5GB/40.0GB (66.2%)\n      della-i14g3 (GPU 0): 26.5GB/40.0GB (66.2%)\n      della-i14g3 (GPU 1): 26.5GB/40.0GB (66.2%)\n\n                                     Notes\n================================================================================\n  * This job only used 6% of the 256GB of total allocated CPU memory. For\n    future jobs, please allocate less memory by using a Slurm directive such\n    as --mem-per-cpu=1G or --mem=10G. This will reduce your queue times and\n    make the resources available to other users. For more info:\n      https://researchcomputing.princeton.edu/support/knowledge-base/memory\n\n  * For additional job metrics including metrics plotted against time:\n    https://mydella.princeton.edu/pun/sys/jobstats\n</code></pre>"},{"location":"#which-metrics-does-jobstats-make-available","title":"Which metrics does Jobstats make available?","text":"<p>Job-level metrics:</p> <ul> <li>CPU Utilization</li> <li>CPU Memory Utilization</li> <li>GPU Utilization</li> <li>GPU Memory\u00a0Utilization</li> <li>GPU Power Usage</li> <li>GPU Temperature</li> </ul> <p>Node-level metrics:</p> <ul> <li>CPU Percentage Utilization</li> <li>Total Memory Utilization</li> <li>Mean Frequency Over All CPUs</li> <li>NFS Statistics</li> <li>Local Disc R/W</li> <li>GPFS Bandwidth Statistics</li> <li>Local Disc IOPS</li> <li>GPFS Operations per Second Statistics</li> <li>Infiniband Throughput</li> <li>Infiniband Packet Rate</li> <li>Infiniband Errors</li> </ul> <p>The following image shows the Grafana dashboard for an example GPU job:</p> <p></p>"},{"location":"#other-job-monitoring-platforms","title":"Other job monitoring platforms","text":"<p>Consider these alternatives to Jobstats:</p> <ul> <li>XDMod (SUPReMM)</li> <li>LLload</li> <li>jobperf</li> <li>TACC Stats</li> <li>REMORA</li> </ul>"},{"location":"#want-to-use-jobstats-at-your-institution","title":"Want to use Jobstats at your institution?","text":"<p>Proceed to the next section where we illustrate the setup of the platform.</p>"},{"location":"contributions/","title":"Contributions","text":"<p>Contributions to the Jobstats platform and its tools are welcome. To work with the code, build a Conda environment:</p> <pre><code>$ conda create --name jobstats-dev requests blessed ruff pytest-mock mkdocs-material -c conda-forge\n</code></pre> <p>Be sure that the tests are passing before making a pull request:</p> <pre><code>(jobstats-env) $ pytest\n</code></pre>"},{"location":"publications/","title":"Publications","text":"<ul> <li>PEARC 2024 poster: PDF</li> <li>PEARC 2023 talk: PDF</li> <li>PEARC 2023 paper: PDF</li> </ul>"},{"location":"support/","title":"Support","text":"<p>For assistance with setting up the Jobstats platform, please post an issue on the Jobstats GitHub repository: https://github.com/PrincetonUniversity/jobstats</p>"},{"location":"setup/cgroups/","title":"CPU Job Statistics","text":"<p>Slurm has to be configured to track job accounting data via the cgroup plug-in. This requires the following line in <code>slurm.conf</code>:</p> <pre><code>JobAcctGatherType=jobacct_gather/cgroup\n</code></pre> <p>The above is in addition to the other usual cgroup-related plug-ins/settings:</p> <pre><code>ProctrackType=proctrack/cgroup\nTaskPlugin=affinity,cgroup\n</code></pre> <p>Slurm will then create two top-level cgroup directories for each job, one for CPU utilization and one for CPU memory. Within each directory there will be subdirectories: <code>step_extern</code>, <code>step_batch</code>, <code>step_0</code>, <code>step_1</code>, and so on. Within these directories one finds <code>task_0</code>, <code>task_1</code>, and so on. These cgroups are scraped by a cgroup exporter. The table below lists all of the collected fields:</p> Name Description Type <code>cgroup_cpu_system_seconds</code> Cumulative CPU system seconds for jobid gauge <code>cgroup_cpu_total_seconds</code> Cumulative CPU total seconds for jobid gauge <code>cgroup_cpu_user_seconds</code> Cumulative CPU user seconds for jobid gauge <code>cgroup_cpus</code> Number of CPUs in the jobid gauge <code>cgroup_memory_cache_bytes</code> Memory cache used in bytes gauge <code>cgroup_memory_fail_count</code> Memory fail count gauge <code>cgroup_memory_rss_bytes</code> Memory RSS used in bytes gauge <code>cgroup_memory_total_bytes</code> Memory total given to jobid in bytes gauge <code>cgroup_memory_used_bytes</code> Memory used in bytes gauge <code>cgroup_memsw_fail_count</code> Swap fail count gauge <code>cgroup_memsw_total_bytes</code> Swap total given to jobid in bytes gauge <code>cgroup_memsw_used_bytes</code> Swap used in bytes gauge <code>cgroup_uid</code> UID number of user running this job gauge <p>The cgroup exporter used here is based on the exporter by Trey Dock [1] with additional parsing of the <code>jobid</code>, <code>steps</code>, <code>tasks</code> and <code>UID</code> number. This produces an output that resembles (e.g., for system seconds):</p> <pre><code>cgroup_cpu_system_seconds{jobid=\"247463\", step=\"batch\", task=\"0\"}\n160.92\n</code></pre> <p>Note that the UID of the owning user is stored as a gauge in <code>cgroup_uid</code>:</p> <pre><code>cgroup_uid{jobid=\"247463\"}\n334987\n</code></pre> <p>This is because accounting is job-oriented and having a UID of the user as a label would needlessly increase the cardinality of the data in Prometheus. All other fields are alike with <code>jobid</code>, <code>step</code> and <code>task</code> labels.</p> <p>The totals for a job have an empty <code>step</code> and <code>task</code>, for example:</p> <pre><code>cgroup_cpu_user_seconds{jobid=\"247463\", step=\"\", task=\"\"}\n202435.71\n</code></pre> <p>This is due to the organization of the cgroup hierarchy. Consider the directory:</p> <pre><code>/sys/fs/cgroup/cpu,cpuacct/slurm/uid_334987\n</code></pre> <p>Within this directory, one finds the following subdirectories:</p> <pre><code>job_247463/cpuacct.usage_user\njob_247463/step_extern/cpuacct.usage_user\njob_247463/step_extern/task_0/cpuacct.usage_user\n</code></pre> <p>This is the data most often retrieved and parsed for overall job efficiency which is why by default the <code>cgroup_exporter</code> does not parse <code>step</code> or <code>task</code> data. To collect all of it, add the <code>--collect.fullslurm option</code>. We run the <code>cgroup_exporter</code> with these options:</p> <pre><code>/usr/sbin/cgroup_exporter --config.paths /slurm --collect.fullslurm\n</code></pre> <p>The <code>--config.paths /slurm</code> has to match the path used by Slurm under the top cgroup directory. This is usually a path that is something like <code>/sys/fs/cgroup/memory/slurm</code>.</p>"},{"location":"setup/gpu_node_scripts/","title":"GPU Job Statistics","text":"<p>GPU metrics (currently only NVIDIA) are collected by the Jobstats GPU exporter which was based on the exporter by Rohit Agarwal [1]. The main local changes were to add the handling of Multi-Instance GPUs (MIG) and two additional gauge metrics: <code>nvidia_gpu_jobId</code> and <code>nvidia_gpu_jobUid</code>. The table below lists all of the collected GPU fields.</p> Name Description Type <code>nvidia_gpu_duty_cycle</code> GPU utilization gauge <code>nvidia_gpu_memory_total_bytes</code> Total memory of the GPU device in bytes gauge <code>nvidia_gpu_memory_used_bytes</code> Memory used by the GPU device in bytes gauge <code>nvidia_gpu_num_devices</code> Number of GPU devices gauge gauge <code>nvidia_gpu_power_usage_milliwatts</code> Power usage of the GPU device in milliwatts gauge <code>nvidia_gpu_temperature_celsius</code> Temperature of the GPU device in Celsius gauge <code>nvidia_gpu_jobId</code> JobId number of a job currently using this GPU as reported by Slurm gauge <code>nvidia_gpu_jobUid</code> UID number of user running jobs on this GPU gauge <p>Note</p> <p>Note that the approach described here is not appropriate for clusters that allow for GPU sharing (e.g., sharding).</p>"},{"location":"setup/gpu_node_scripts/#gpu-job-ownership-helper","title":"GPU Job Ownership Helper","text":"<p>In order to correctly track which GPU is assigned to which <code>jobid</code>, we use Slurm prolog and epilog scripts to create files in <code>/run/gpustat</code>. These files are named either after GPU ordinal number (0, 1, ...) or, in the case of Multi-Instance GPUs (MIG), MIG-UUID. These files contain the space-separated <code>jobid</code> and <code>UID</code> number of the user, for example:</p> <pre><code>$ cat /run/gpustat/MIG-265a219d-a49f-578a-825d-222c72699c16\n45916256 262563\n</code></pre> <p>These two scripts can be found in the <code>slurm</code> directory of the Jobstats GitHub repository. For example, <code>slurm/epilog.d/gpustats_helper.sh</code> could be installed as <code>/etc/slurm/epilog.d/gpustats_helper.sh</code> and <code>slurm/prolog.d/gpustats_helper.sh</code> as <code>/etc/slurm/prolog.d/gpustats_helper.sh</code> with these <code>slurm.conf</code> statements:</p> <pre><code>Prolog=/etc/slurm/prolog.d/*.sh\nEpilog=/etc/slurm/epilog.d/*.sh\n</code></pre> <p>For efficiency and simplicity, <code>JobId</code> and <code>jobUid</code> are collected from files in <code>/run/gpustat/0</code> (for GPU 0), <code>/run/gpustat/1</code> (for GPU 1), and so on. For example:</p> <pre><code>$ cat /run/gpustat/0\n247609 223456\n</code></pre> <p>In the above, the first number is the <code>jobid</code> and the second is the <code>UID</code> number of the owning user. These are created with Slurm <code>prolog.d</code> and <code>epilog.d</code> scripts that can be found in the Jobstats GitHub repository.</p>"},{"location":"setup/grafana/","title":"Grafana","text":"<p>The four exporters lead to a wealth of data in the Prometheus database. To visualize this data, the Grafana visualization toolkit is used. To setup Grafana follow the directions at grafana.com.</p> <p>The Grafana dashboard JSON file, which uses all of the exporters, is included in the <code>grafana</code> subdirectory in the Jobstats GitHub repository. The dashboard expects one parameter, <code>jobid</code>. As it may not be easy to find the time range of the job, we also use an OnDemand helper that generates the correct time range given a <code>jobid</code> (see the next section).</p> <p>The following job-level metrics are available in both Grafana and the <code>jobstats</code> command:</p> <ul> <li>CPU Utilization</li> <li>CPU Memory Utilization</li> <li>GPU Utilization </li> <li>GPU Memory\u00a0Utilization </li> </ul> <p>The following additional job-level metrics are exposed only in Grafana:</p> <ul> <li>GPU Power Usage</li> <li>GPU Temperature </li> </ul> <p>Finally, the following additional node-level metrics are exposed only in Grafana:</p> <ul> <li>CPU Percentage Utilization</li> <li>Total Memory Utilization</li> <li>Mean Frequency Over All CPUs</li> <li>NFS Statistics</li> <li>Local Disc R/W</li> <li>GPFS Bandwidth Statistics</li> <li>Local Disc IOPS</li> <li>GPFS Operations per Second Statistics </li> <li>Infiniband Throughput</li> <li>Infiniband Packet Rate</li> <li>Infiniband Errors</li> </ul> <p>The complete Grafana interface for the Jobstats platform is composed of plots of the time history of the seventeen quantities above. This graphical interface is used for detailed investigations such as troubleshooting failed jobs, identifying jobs with CPU memory leaks, intermittent GPU usage, load imbalance, and for understanding the anomalous behavior of system hardware.</p> <p>Note</p> <p>Eleven of the seventeen metrics above are node-level. This means that if multiple jobs are running on the same node then it will not be possible to disentangle the data. To use these metrics to troubleshoot jobs, the job should allocate the entire node.</p> <p>The following image illustrates what the dashboard looks like in use:</p> <p></p>"},{"location":"setup/jobstats_command/","title":"The <code>jobstats</code> command","text":"<p>The last step in setting up the Jobstats platform is installing the <code>jobstats</code> command. This command generates the job efficiency report. For completed jobs, the data is available in the Slurm database. For actively running jobs, the Prometheus database must be queried to obtain the data needed to generate the report.</p>"},{"location":"setup/jobstats_command/#installation","title":"Installation","text":"<p>The installation requirements for <code>jobstats</code> are Python 3.6+, Requests 2.20+ and (optionally) blessed 1.17+ which can be used for coloring and styling text.</p> <p>The necessary software can be installed as follows:</p> Ubuntucondapip <pre><code>$ apt-get install python3-requests python3-blessed\n</code></pre> <pre><code>$ conda create --name js-env python=3.7 requests blessed -c conda-forge\n</code></pre> <pre><code>$ python3 -m venv .venv\n$ source .venv/bin/activate\n(.venv) $ pip3 install requests blessed\n</code></pre> <p>The four files needed to run the <code>jobstats</code> command are available in the Jobstats GitHub repository:</p> <pre><code>$ ls -l /usr/local/bin\nconfig.py\njobstats\njobstats.py\noutput_formatters.py\n</code></pre> <p>Remember to change the permissions to make <code>jobstats</code> executable. An overview of the command can be seen by looking at the help menu:</p> <pre><code>$ jobstats --help\n</code></pre> <p>The command takes the <code>jobid</code> as the only required argument:</p> <pre><code>$ jobstats 12345678\n</code></pre>"},{"location":"setup/jobstats_command/#configuration-file","title":"Configuration File","text":"<p>The <code>jobstats</code> command requires a <code>config.py</code> configuration file. Use <code>config.py</code> in the Jobstats GitHub repository as the starting point for your configuration.</p> <p>The first entry in <code>config.py</code> is for the Prometheus server:</p> <pre><code># prometheus server address and port\nPROM_SERVER = \"http://vigilant2:8480\"\n</code></pre> <p>The number of seconds between measurements by the exporters on the compute nodes:</p> <pre><code># number of seconds between measurements\nSAMPLING_PERIOD = 30\n</code></pre> <p>The value above should match that in the Prometheus configuration, i.e., <code>scrape_interval: 30s</code>.</p> <p>One can use the Python <code>blessed</code> package to produce bold and colorized text. This helps to draw attention to specific lines of the report. This part of the configuration sets the various thresholds:</p> <pre><code># threshold values for red versus black notes\nGPU_UTIL_RED   = 15  # percentage\nGPU_UTIL_BLACK = 25  # percentage\nCPU_UTIL_RED   = 65  # percentage\nCPU_UTIL_BLACK = 80  # percentage\nTIME_EFFICIENCY_RED   = 40  # percentage\nTIME_EFFICIENCY_BLACK = 70  # percentage\n</code></pre> <p>For instance, if the overal GPU utilization is less than 15% then it will be displayed in bold red text. Search the conditions in the example notes in <code>config.py</code> to see how the other values are used.</p> <p>The following threshold can be used to trigger notes about excessive CPU memory usage:</p> <pre><code>MIN_MEMORY_USAGE = 70  # percentage\n</code></pre> <p>Notes can be suppressed if the run time of the job is less than the following threshold:</p> <pre><code>MIN_RUNTIME_SECONDS = 10 * SAMPLING_PERIOD  # seconds\n</code></pre> <p>Use <code>CLUSTER_TRANS</code> to convert informal cluster names to the name that is used in the Slurm database. For instance, if the <code>tiger</code> cluster is replaced by the <code>tiger2</code> cluster then use:</p> <pre><code>CLUSTER_TRANS = {\"tiger\":\"tiger2\"}\nCLUSTER_TRANS_INV = dict(zip(CLUSTER_TRANS.values(), CLUSTER_TRANS.keys()))\n</code></pre> <p>This will allow users to specify <code>tiger</code> as the cluster while internally the value <code>tiger2</code> is used when querying the Slurm database.</p> <p>One can trim long job names:</p> <pre><code># maximum number of characters to display in jobname\nMAX_JOBNAME_LEN = 64\n</code></pre> <p>Notes concerning excessive CPU memory allocations may require the values of the default memory per CPU-core:</p> <pre><code># default CPU memory per core in bytes for each cluster\n# if unsure then use memory per node divided by cores per node\nDEFAULT_MEM_PER_CORE = {\"adroit\":3_355_443_200,\n                        \"della\":4_194_304_000,\n                        \"stellar\":7_864_320_000,\n                        \"tiger\":4_294_967_296,\n                        \"traverse\":7_812_500_000}\n</code></pre> <p>In Python, one can specify a large number like 1 billion as <code>1_000_000_000</code>.</p> <p>Similarly, the number of CPU-cores per node for each cluster is required for certain notes to be triggered:</p> <pre><code># number of CPU-cores per node for each cluster\n# this will eventually be replaced with explicit values for each node\nCORES_PER_NODE = {\"adroit\":32,\n                  \"della\":28,\n                  \"stellar\":96,\n                  \"tiger\":40,\n                  \"traverse\":32}\n</code></pre>"},{"location":"setup/jobstats_command/#custom-job-notes","title":"Custom Job Notes","text":"<p>Institutions that use the Jobstats platform have the ability to write custom notes in <code>config.py</code> that can appear at the bottom of the job efficiency reports. Here is a simple example that makes the user aware of the Grafana dashboard:</p> <pre><code>                                  Notes\n============================================================================\n  * For additional job metrics including metrics plotted against time:\n      https://mytiger.princeton.edu/pun/sys/jobstats\n</code></pre> <p>Job notes can be used to provide information and to guide users toward solving underutilization issues such low GPU utilization or excessive CPU memory allocations.</p> <p>Each note is Python code that is composed of three items: (1) a <code>condition</code>, (2) the actual <code>note</code>, and (3) the <code>style</code>. The <code>condition</code> is a Python string that gets evaluated to <code>True</code> or <code>False</code> when <code>jobstats</code> is ran. The <code>note</code> is the text to be displayed. Lastly, the <code>style</code> sets the formatting which is either <code>normal</code>, <code>bold</code>, or <code>bold-red</code>.</p> <p>Consider the following note in <code>config.py</code>:</p> <pre><code>condition = '(self.js.cluster == \"tiger\")'\nnote = (\"For additional job metrics including metrics plotted against time:\",\n        \"https://mytiger.princeton.edu/pun/sys/jobstats\")\nstyle = \"normal\"\nNOTES.append((condition, note, style))\n</code></pre> <p>The note above will be displayed by <code>jobstats</code> for all jobs that ran on the <code>tiger</code> cluster.</p> <p>Much more sophisicated and useful notes can be constructed. For more ideas and examples, see the many notes that appear in <code>config.py</code> in the Jobstats GitHub repository.</p> <p>Notes can contain Slurm directives and URLs. These items are automatically displayed on a separate line with additional indentation.</p> <p>Warning</p> <p>System administrators should not give users the ability to add notes to <code>config.py</code> since in principle they could write malicious code that would be executed when <code>jobstats</code> is ran.</p> <p>If you decide not to use notes then keep <code>NOTES = []</code> in <code>config.py</code> but remove everything  below that line.</p> <p>Below are some example notes that are possible:</p> <pre><code>  * This job ran on the mig partition where each job is limited to 1 MIG\n    GPU, 1 CPU-core, 10 GB of GPU memory and 32 GB of CPU memory. A MIG GPU\n    is about 1/7th as powerful as an A100 GPU. Please continue using the mig\n    partition when possible. For more info:\n      https://researchcomputing.princeton.edu/systems/della#gpus\n\n  * This job completed while only needing 19% of the requested time which\n    was 2-00:00:00. For future jobs, please decrease the value of the --time\n    Slurm directive. This will lower your queue times and allow the Slurm\n    job scheduler to work more effectively for all users. For more info:\n      https://researchcomputing.princeton.edu/support/knowledge-base/slurm\n\n  * This job did not use the GPU. Please resolve this before running\n    additional jobs. Wasting resources prevents other users from getting\n    their work done and it causes your subsequent jobs to have a lower\n    priority. Is the code GPU-enabled? Please consult the documentation for\n    the code. For more info:\n      https://researchcomputing.princeton.edu/support/knowledge-base/gpu-computing\n\n  * This job only used 15% of the 100GB of total allocated CPU memory.\n    Please consider allocating less memory by using the Slurm directive\n    --mem-per-cpu=3G or --mem=18G. This will reduce your queue times and\n    make the resources available to other users. For more info:\n      https://researchcomputing.princeton.edu/support/knowledge-base/memory\n\n  * This job ran on a large-memory (datascience) node but it only used 117\n    GB of CPU memory. The large-memory nodes should only be used for jobs\n    that require more than 190 GB. Please allocate less memory by using the\n    Slurm directive --mem-per-cpu=9G or --mem=150G. For more info:\n      https://researchcomputing.princeton.edu/support/knowledge-base/memory\n\n  * The CPU utilization of this job (24%) is approximately equal to 1\n    divided by the number of allocated CPU-cores (1/4=25%). This suggests\n    that you may be running a code that can only use 1 CPU-core. If this is\n    true then allocating more than 1 CPU-core is wasteful. Please consult\n    the documentation for the software to see if it is parallelized. For\n    more info:\n      https://researchcomputing.princeton.edu/support/knowledge-base/parallel-code\n\n  * This job did not use the CPU. This suggests that something went wrong at\n    the very beginning of the job. Check your Slurm script for errors and\n    look for useful information in the file slurm-46987157.out if it exists.\n\n  * The Tiger cluster is intended for jobs that require multiple nodes. This\n    job ran in the serial partition where jobs are assigned the lowest\n    priority. On Tiger, a job will run in the serial partition if it only\n    requires 1 node. Consider carrying out this work elsewhere.\n\n  * For additional job metrics including metrics plotted against time:\n      https://mystellar.princeton.edu/pun/sys/jobstats  (VPN required off-campus)\n\n  * For additional job metrics including metrics plotted against time:\n      https://stats.rc.princeton.edu  (VPN required off-campus)\n</code></pre> <p>Each institution that uses Jobstats is encouraged to write custom notes for their users. We have received feedback indicating that this is one of the most useful features of the entire Jobstats ecosystem.</p>"},{"location":"setup/node/","title":"Node Statistics","text":"<p>The Prometheus <code>node_exporter</code> should be setup to run on every compute node. This allows the Jobstats platform to obtain basic node metrics such as total memory available, memory usage, CPU frequencies, NFS statistics, Infiniband statistics and many other potentially useful quantities. Spectrum Scale/GPFS statistics are collected with a custom Python-based exporter.</p> <p>Proceed to the next section on Job Summaries.</p>"},{"location":"setup/ood/","title":"Open OnDemand Jobstats Helper","text":"<p>See the <code>ood-jobstats-helper</code> directory in the Jobstats GitHub repository. This directory contains an Open OnDemand app that, given a <code>jobid</code>, uses <code>sacct</code> to generate a full Grafana URL with the <code>jobid</code>, start time and end time.</p>"},{"location":"setup/overview/","title":"Setup Overview","text":"<p>Below is an outline of the steps that need to be taken to setup the Jobstats platform for a Slurm cluster:</p> <ol> <li>Switch to cgroup-based job accounting from Linux process accounting </li> <li>Setup the exporters: cgroup, node, GPU (on the nodes) and, optionally, GPFS (centrally)</li> <li>Setup the <code>prolog.d</code> and <code>epilog.d</code> scripts on the GPU nodes</li> <li>Setup the Prometheus server and configure it to scrape the data from the compute nodes</li> <li>Setup the <code>slurmctldepilog.sh</code> script for long-term job summary retention</li> <li>Lastly, configure the Grafana interface and Open OnDemand</li> </ol> <p>A single standard server has proven to be sufficient for a data center with 100,000 CPU-cores and 1000 GPUs.</p> <p>Proceed to the next section on cgroup-based job accounting.</p>"},{"location":"setup/prometheus/","title":"Prometheus","text":"<p>Prometheus is a monitoring system and time series database. For setup, follow the directions at prometheus.io. The four Prometheus exporters required by the Jobstats platform were discussed in the previous sections.</p>"},{"location":"setup/prometheus/#basic-prometheus-configuration","title":"Basic Prometheus Configuration","text":"<p>What follows is an example of production configuration used for the Tiger cluster that has both CPU and GPU nodes:</p> <pre><code>---\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n  external_labels:\n    monitor: master\n- job_name: Tiger Nodes\n  scrape_interval: 30s\n  scrape_timeout: 30s\n  file_sd_configs:\n  - files:\n    - \"/etc/prometheus/local_files_sd_config.d/tigernodes.json\"\n  metric_relabel_configs:\n  - source_labels:\n    - __name__\n    regex: \"^go_.*\"\n    action: drop\n- job_name: TigerGPU Nodes\n  scrape_interval: 30s\n  scrape_timeout: 30s\n  file_sd_configs:\n  - files:\n    - \"/etc/prometheus/local_files_sd_config.d/tigergpus.json\"\n  metric_relabel_configs:\n  - source_labels:\n    - __name__\n    regex: \"^go_.*\"\n    action: drop\n</code></pre> <p>The <code>tigernode.json</code> file looks like:</p> <pre><code> [\n   {\n     \"labels\": {\n       \"cluster\": \"tiger\"\n     },\n     \"targets\": [\n       \"tiger-h19c1n10:9100\",\n       \"tiger-h19c1n10:9306\",\n       ...\n     ]\n   }\n ]\n</code></pre> <p>Both <code>node_exporter</code> (port 9100) and <code>cgroup_exporter</code> (port 9306) are listed for all of the nodes in <code>tigernode.json</code>. The file <code>tigergpus.json</code> looks very similar except that it collects data from <code>nvidia_gpu_prometheus_exporter</code> on port 9445. Note the additional <code>cluster</code> label.</p>"},{"location":"setup/smail/","title":"Procedure for Modifying User Email Reports","text":"<p>The <code>jobstats</code> command should be configured to replace <code>smail</code>, which is the Slurm executable used for sending email reports. To make this change, edit <code>slurm.conf</code> as follows:</p> <pre><code>MailProg=/usr/local/bin/jobstats_mail.sh\n</code></pre> <p>The <code>jobstats_mail.sh</code> script is available in the <code>slurm</code> directory of the Jobstats GitHub repository. This script sets the <code>content-type</code> to <code>text/html</code> so that the email is sent using a fixed-width font.</p> <p>As always, to receive an email report, users must include the appropriate Slurm directive in their scripts:</p> <pre><code>#SBATCH --mail-type=end\n</code></pre> <p>Note</p> <p>When the run time of the job is less than the sampling period of the Prometheus exporters (which is typically 30 seconds), <code>jobstats</code> will call <code>seff</code> to generate the job report.</p>"},{"location":"setup/summaries/","title":"Generating Job Summaries","text":"<p>When all four exporters are used, the Prometheus database stores 17 metrics every N seconds for each job. It is recommended to retain this detailed data for several months or longer. This data can be visualized using the Grafana dashboard. After some amount of time it makes sense to purge the detailed data while keeping only a summary (i.e., CPU/GPU utilization and memory usage per node). The summary data can also be used in place of the detailed data when generating efficiency reports.</p> <p>A summary of individual job statistics is generated at job completion and stored in the Slurm database in the <code>AdminComment</code> field. This is done by a <code>slurmctld</code> epilog script that runs at job completion. For example, in <code>slurm.conf</code>:</p> <pre><code>EpilogSlurmctld=/usr/local/sbin/slurmctldepilog.sh\n</code></pre> <p>The script is available in the Jobstats GitHub repository. For storage efficiency and convenience, the JSON job summary data is gzipped and base64 encoded before being stored in the <code>AdminComment</code> field of the Slurm database.</p> <p>The impact on the database size due to this depends on job sizes. For an institution with 100,000 CPU-cores, for small jobs the <code>AdminComment</code> field tends to average under 50 characters per entry with a maximum under 1500 while for large jobs the maximum length is around 5000.</p> <p>For processing old jobs where the <code>slurmctld</code> epilog script did not run or for jobs where it failed, there is a per cluster ingest Jobstats service. This is a Python-based script running on the <code>slurmdbd</code> host as a <code>systemd</code> timer and service, acting to query and modify the Slurm database directly. The script (<code>ingest_jobstats.py</code>) and <code>systemd</code> timer and service scripts are in the <code>slurm</code> directory of the Jobstats GitHub repository.</p> <p>Below is an example job summary for a GPU job:</p> <pre><code>$ jobstats 12345678 -j\n{\n    \"gpus\": 1,\n    \"nodes\": {\n        \"della-k1g1\": {\n            \"cpus\": 12,\n            \"gpu_total_memory\": {\n                \"1\": 85899345920\n            },\n            \"gpu_used_memory\": {\n                \"1\": 83314868224\n            },\n            \"gpu_utilization\": {\n                \"1\": 98.6\n            },\n            \"total_memory\": 137438953472,\n            \"total_time\": 57620.8,\n            \"used_memory\": 84683702272\n        }\n    },\n    \"total_time\": 50944\n}\n</code></pre>"},{"location":"tools/gpudash/","title":"GPU Dashboard","text":"<p>The <code>gpudash</code> command generates a text-based dashboard of the GPU utilization across a cluster in the form of a 2-dimensional grid. Each cell displays the utilization from 0-100% along with the username associated with each allocated GPU. Cells are colored according to their utilization values making it easy to identify jobs with low or high GPU utilization. The <code>gpudash</code> command can also be used to check for available GPUs.</p> <p>By default, the dashboard has seven columns and a number of rows equal to the number of GPUs on the cluster. Each column is evenly spaced in time by N minutes. We find a good choice is N=10 minutes which leads to data being shown over an hour. The <code>cron</code> utility can be used to achieve this. The rows are labeled by the node name and the GPU index while the columns are labeled by time.</p> <p>The <code>gpudash</code> command works by making the three queries to the Prometheus server every N minutes. A Python script is used to extract the information from the three generated JSON files and append this data to the files read by <code>gpudash</code>. The <code>UID</code> for each user is matched with its corresponding username. The <code>jobid</code> is not required but it can be useful for troubleshooting.</p> <p>Nodes that are down, or in a state which makes them unavailable, are not shown in the dashboard. Labels can be added to mark reserved nodes or special-purpose nodes.</p>"},{"location":"tools/gpudash/#installation","title":"Installation","text":"<p>The installation requirements for <code>gpudash</code> are Python 3.6+ and version 1.17+ of the Python <code>blessed</code> package which is used for creating colored text and backgrounds. The Python code and instructions are available at https://github.com/PrincetonUniversity/gpudash.</p>"},{"location":"tools/job_defense_shield/","title":"Job Defense Shield","text":"<p>High-performance computing clusters often serve a large number of users who posses a range of knowledge and skills. This leads to individuals misusing the resources due to mistakes, misunderstandings, expediency, and related issues. To combat jobs that waste or misuse the resources, a battery of alerts can be configured. While such alerts can be configured in Prometheus, the most flexible and powerful solution is external software.</p> <p>Job Defense Shield is a Python code for sending automated email alerts to users and for creating reports for system administrators. As discussed above, summary statistics for each completed job are stored in a compressed format in the <code>AdminComment</code> field in the Slurm database. The software described here works by calling the Slurm <code>sacct</code> command while requesting several fields including <code>AdminComment</code>. The <code>sacct</code> output is stored in a <code>pandas</code> dataframe for processing.</p> <p>Automated email alerts to users are available for these cases:</p> <ul> <li>CPU or GPU jobs with 0% utilization (see email below)</li> <li>Heavy users with low mean CPU or GPU efficiency</li> <li>Jobs that allocate excess CPU memory (see email below)</li> <li>Serial jobs that allocate multiple CPU-cores</li> <li>Users that routinely run with excessive time limits</li> <li>Jobs that could have used a smaller number of nodes</li> <li>Jobs that could have used less powerful GPUs</li> <li>Jobs thar ran on specialized nodes but did not need to</li> </ul> <p>All of the instances in the list above can be formulated as a report for system administrators. The most popular reports for system administrators are:</p> <ul> <li>A list of users (and their jobs) with the most GPU-hours at 0% utilization</li> <li>A list of the heaviest users with low CPU/GPU utilization</li> <li>A list of users that are over-allocating the most CPU memory</li> <li>A list of users that are over-allocating the most time</li> </ul> <p>The Python code is written using object-oriented programming techniques which makes it easy to create new alerts and reports.</p>"},{"location":"tools/job_defense_shield/#example-emails","title":"Example Emails","text":"<p>Below is an example email for the automatic cancellation of a GPU job with 0% utilization:</p> <pre><code>Hi Alan,\n\nThe jobs below have been cancelled because they ran for nearly 2 hours at 0% GPU\nutilization:\n\n     JobID    Cluster  Partition    State    GPUs-Allocated GPU-Util  Hours\n    60131148   della      llm     CANCELLED         4          0%      2.0  \n    60131741   della      llm     CANCELLED         4          0%      1.9  \n\nSee our GPU Computing webpage for three common reasons for encountering zero GPU\nutilization:\n\n    https://&lt;your-institution&gt;.edu/knowledge-base/gpu-computing\n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre> <p>Below is an example email to a user that is requesting too much CPU memory:</p> <pre><code>Hi Alan,\n\nBelow are your jobs that ran on BioCluster in the past 7 days:\n\n     JobID   Memory-Used  Memory-Allocated  Percent-Used  Cores  Hours\n    5761066      2 GB          100 GB            2%         1      48\n    5761091      4 GB          100 GB            4%         1      48\n    5761092      3 GB          100 GB            3%         1      48\n\nIt appears that you are requesting too much CPU memory for your jobs since\nyou are only using on average 3% of the allocated memory. For help on\nallocating CPU memory with Slurm, please see:\n\n    https://&lt;your-institution&gt;.edu/knowledge-base/memory\n\nReplying to this automated email will open a support ticket with Research\nComputing. \n</code></pre>"},{"location":"tools/job_defense_shield/#usage","title":"Usage","text":"<p>The software has a <code>check</code> mode that shows on which days a given user received an alert of a given type. Users that appear to be ignoring the email alerts can be contacted directly. Emails to users are most effective when sent sparingly. For this reason, there is a command-line parameter to specify the amount of time that must pass before the user can receive another email of the same nature.</p> <p>The example below shows how the script is called to notify users in the top N by usage with low CPU or GPU efficiencies over the last week:</p> <pre><code>$ job_defense_shield --low-xpu-efficiencies --days=7 --email\n</code></pre> <p>The default thresholds are 60% and 15% for CPU and GPU utilization, respectively, and N=15.</p>"},{"location":"tools/job_defense_shield/#installation","title":"Installation","text":"<p>The installation requirements for Job Defense Shield are Python 3.6+ and version 1.2+ of the Python <code>pandas</code> package. The <code>jobstats</code> command is also required if one wants to examine actively running jobs such as when looking for jobs with zero GPU utilization. The Python code, example alerts and emails, and instructions are available in the GitHub repository.</p>"},{"location":"tools/overview/","title":"Overview of Jobstats Tools","text":"<p>The Jobstats platform provides a rich foundation for building powerful tools. In this section we describe four such tools:</p> <ul> <li>Job Defense Shield</li> <li>GPU Dashboard</li> <li>Utilization Reports</li> <li>reportseff</li> </ul>"},{"location":"tools/reportseff/","title":"<code>reportseff</code>","text":"<p>The <code>reportseff</code> utility wraps <code>sacct</code> to provide a cleaner user experience when interrogating Slurm job efficiency values for multiple jobs. In addition to multiple jobids, <code>reportseff</code> accepts Slurm output files as arguments and parses the jobid from the filename. Some <code>sacct</code> options are further wrapped or extended to simplify common operations. The output is a table with entries colored based on high/low utilization values. The columns and formatting of the table can be customized based on command line options.</p> <p>A limit to the previous tools is that they provide information on a single job at a time in great detail. Another common use case is to summarize job efficiency for multiple jobs to gain a better idea of the overall utilization. Summarized reporting is especially useful with array jobs and workflow managers which interface with Slurm. In these cases, running <code>seff</code> or <code>jobstats</code> becomes burdensome.</p>"},{"location":"tools/reportseff/#usage","title":"Usage","text":"<p>The <code>reportseff</code> tool accepts jobs as jobids, Slurm output files, and directories containing Slurm output files:</p> <pre><code>$ reportseff 123 124      # get information on jobs 123 and 124\n$ reportseff {123..133}   # get information on jobs 123 to 133\n$ reportseff jobname*     # check output files starting with jobname\n$ reportseff slurm_out/   # look for output files in the slurm_out directory\n</code></pre> <p>The ability to link Slurm outputs with job status simplifies locating problematic jobs and cleaning up their outputs. The <code>reportseff</code> utility extends some of the <code>sacct</code> options. The start and end time can accept any format accepted by <code>sacct</code>, as well as a custom format, specified as a comma separated list of key/value pairs. For example:</p> <pre><code>$ reportseff --since now-27hours   # equivalent to\n$ reportseff --since d=1,h=3       # 1 day, 3 hours\n</code></pre> <p>Filtering by job state is expanded with <code>reportseff</code> to specify states to exclude. This filtering combined with accepting output files helps in cleaning up failed output jobs:</p> <pre><code>$ reportseff --not-state CD     # not completed\n             --since d=1 \\      # today\n             --format=jobid \\   # just get file name\n             my_failing_job* \\  # only from these outputs\n             | xargs grep \"output:\"\n</code></pre> <p>The last piece of the pipeline above find lines with the output directive to examine or delete. The format option can accept a comma-separated list of column names or additional columns can be appended to the default values. Appending prevents the need to add in the same, default columns on every invocation.</p> <p>While the above features are available for any Slurm system, when Jobstats information is present in the <code>AdminComment</code>, the multi-node resource utilization is updated with the more accurate Jobstats values and GPU utilization is also provided. This additional information is controlled with the <code>--node</code> and <code>--node-and-gpu</code> options.</p> <p>A sample workflow with <code>reportseff</code> is to run a series of jobs, each producing an output file. Run <code>reportseff</code> on the output directory to determine the utilization and state of each job. Jobs with low utilization or failure can be examined more closely by copy/pasting the Slurm output filename from the first column. Outputs from failed jobs can be cleaned automatically with a version of the command piping above. Combining with watch and aliases can create powerful monitoring for users:</p> <pre><code># monitor the current directory every 5 minutes\n$ watch -cn 300 reportseff --modified-sort\n\n# monitor the user's efficiency every 10 minutes\n$ watch -cn 600 reportseff --user $USER --modified-sort --format=+jobname\n</code></pre>"},{"location":"tools/reportseff/#installation","title":"Installation","text":"<p>The installation requirements for reportseff are Python 3.7+ and version 6.7+ of the Python <code>click</code> package which is used for creating colored text and command-line parsing. The Python code and instructions are available at https://github.com/troycomi/reportseff.</p>"},{"location":"tools/utilization_reports/","title":"Utilization Reports","text":"<p>This is a tool for sending detailed usage reports to users or group leaders by email.</p> <p>Users can receive an email utilization report upon completion of each job via Slurm directives. Because some users decide not to receive these emails, it is important to periodically send a comprehensive utilization report to each user. As discussed earlier, summary statistics for each completed job are stored in a compressed format in the <code>AdminComment</code> field of the Slurm database. The software described here works by calling <code>sacct</code> while requesting several fields including <code>AdminComment</code>. The <code>sacct</code> output is stored in a <code>pandas</code> dataframe for processing.</p> <p>Each user that ran at least one Slurm job in the specified time interval will receive a report when the software is run. The first part of the report is a table that indicates the overall usage for each cluster. Each row provides the CPU-hours, GPU-hours, number of jobs, and Slurm account(s) and partition(s) that were used by the user.</p> <p>The second part of the report is a detailed table showing for each partition of each cluster the CPU-hours, CPU-rank, CPU-eff, GPU-hours, GPU-rank, GPU-eff and number of jobs. The CPU-rank or GPU-rank indicates the user's usage relative to the other users on the given partition of the cluster. CPU-eff (or GPU-eff) is the overall CPU (or GPU) efficiency which varies from 0-100%. A responsible user will take action when seeing that their rank is high while their efficiency is low. The email report also provides a definition for each reported quantity. The software could be extended by adding queue hours and data storage information to the tables.</p>"},{"location":"tools/utilization_reports/#usage","title":"Usage","text":"<p>The default mode of the software is to send user reports. It can also be used to send reports to those that are responsible for the users such as the principal investigator. This is the so-called <code>sponsors</code> mode. The example below shows how the script is called to generate user reports over the past month which are sent by email:</p> <pre><code>$ utilization_reports --report-type=users --months=1 --email\n</code></pre>"},{"location":"tools/utilization_reports/#installation","title":"Installation","text":"<p>We find a good choice is to send the report once per month. The installation requirements for the software are Python 3.6+ and version 1.2+ of the <code>pandas</code> package. The Python code, example reports, and instructions are available at https://github.com/PrincetonUniversity/monthly_sponsor_reports.</p>"}]}